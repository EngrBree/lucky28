{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)  \n",
    "    df['code'] = df['code'].apply(eval)  # Convert string to list\n",
    "    df['num1'] = df['code'].apply(lambda x: int(x[0]))\n",
    "    df['num2'] = df['code'].apply(lambda x: int(x[1]))\n",
    "    df['num3'] = df['code'].apply(lambda x: int(x[2]))\n",
    "    df['sum'] = df['num1'] + df['num2'] + df['num3']\n",
    "    df['odd_even'] = df['sum'] % 2  # 0 = Even, 1 = Odd\n",
    "    df['big_small'] = (df['sum'] >= 14).astype(int)  # 0 = Small, 1 = Big\n",
    "    \n",
    "    # Feature Engineering (Rolling Mean and Lag Features)\n",
    "    df['rolling_sum_mean'] = df['sum'].rolling(window=3, min_periods=1).mean()\n",
    "    df['lag1_sum'] = df['sum'].shift(1)\n",
    "    df['lag1_odd_even'] = df['odd_even'].shift(1)\n",
    "    df['lag1_big_small'] = df['big_small'].shift(1)\n",
    "\n",
    "    return df[['num1', 'num2', 'num3', 'sum', 'rolling_sum_mean', 'lag1_sum', 'odd_even', 'big_small', 'lag1_odd_even', 'lag1_big_small']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"newlucky28.csv\" \n",
    "df = load_data(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLLING  MISSSING VALUES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Before Handling:\n",
      " num1                0\n",
      "num2                0\n",
      "num3                0\n",
      "sum                 0\n",
      "rolling_sum_mean    0\n",
      "lag1_sum            1\n",
      "odd_even            0\n",
      "big_small           0\n",
      "lag1_odd_even       1\n",
      "lag1_big_small      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values Before Handling:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = ['rolling_sum_mean', 'lag1_sum', 'lag1_odd_even', 'lag1_big_small']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[missing_columns] = df[missing_columns].fillna(df[missing_columns].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values after Handling:\n",
      " num1                0\n",
      "num2                0\n",
      "num3                0\n",
      "sum                 0\n",
      "rolling_sum_mean    0\n",
      "lag1_sum            0\n",
      "odd_even            0\n",
      "big_small           0\n",
      "lag1_odd_even       0\n",
      "lag1_big_small      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values after Handling:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN AND TEST DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (80% Train, 20% Test ensuring 100 draws in test set)\n",
    "test_size = max(0.2, 100 / len(df)) \n",
    "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SCALLING AND ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['sum', 'rolling_sum_mean', 'lag1_sum']\n",
    "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
    "test_df[numerical_features] = scaler.transform(test_df[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "categorical_features = ['odd_even', 'big_small', 'lag1_odd_even', 'lag1_big_small']\n",
    "\n",
    "train_encoded = encoder.fit_transform(train_df[categorical_features])\n",
    "test_encoded = encoder.transform(test_df[categorical_features])\n",
    "\n",
    "train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Concatenate encoded categorical features with numerical features\n",
    "train_df = pd.concat([train_df.drop(columns=categorical_features), train_encoded_df], axis=1)\n",
    "test_df = pd.concat([test_df .drop(columns=categorical_features), test_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in odd_even: [1 0]\n",
      "Unique values in big_small: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# Verify Encoding of Target Labels\n",
    "print(\"Unique values in odd_even:\", df['odd_even'].unique())\n",
    "print(\"Unique values in big_small:\", df['big_small'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING PROCESSED DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store processed datasets\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTORCH DATASET LOADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LotteryDataset(Dataset):\n",
    "    def __init__(self, file_path, target_column):\n",
    "        data = pd.read_csv(file_path)\n",
    "        self.X = torch.tensor(data.drop(columns=[target_column]).values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(data[target_column].values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "def get_dataloaders(train_file, test_file, target_column, batch_size=64):\n",
    "    train_dataset = LotteryDataset(train_file, target_column)\n",
    "    test_dataset = LotteryDataset(test_file, target_column)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders for Odd/Even and Big/Small prediction tasks\n",
    "train_loader_odd_even, test_loader_odd_even = get_dataloaders(\"train.csv\", \"test.csv\", \"odd_even_1\")\n",
    "train_loader_big_small, test_loader_big_small = get_dataloaders(\"train.csv\", \"test.csv\", \"big_small_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train.csv: Index(['num1', 'num2', 'num3', 'sum', 'rolling_sum_mean', 'lag1_sum',\n",
      "       'odd_even_1', 'big_small_1', 'lag1_odd_even_1.0', 'lag1_big_small_1.0'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print(\"Columns in train.csv:\", train_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define MLP Model with Improvements\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 660.7040, Accuracy = 50.00%\n",
      "Epoch 2: Loss = 652.8834, Accuracy = 50.02%\n",
      "Epoch 3: Loss = 650.5686, Accuracy = 50.02%\n",
      "Epoch 4: Loss = 648.7625, Accuracy = 50.45%\n",
      "Epoch 5: Loss = 645.4698, Accuracy = 52.24%\n",
      "Epoch 6: Loss = 638.1241, Accuracy = 54.31%\n",
      "Epoch 7: Loss = 632.1897, Accuracy = 55.76%\n",
      "Epoch 8: Loss = 625.0334, Accuracy = 59.42%\n",
      "Epoch 9: Loss = 619.7885, Accuracy = 61.03%\n",
      "Epoch 10: Loss = 616.1640, Accuracy = 61.94%\n",
      "Epoch 11: Loss = 614.3800, Accuracy = 62.20%\n",
      "Epoch 12: Loss = 611.9861, Accuracy = 62.68%\n",
      "Epoch 13: Loss = 611.4128, Accuracy = 62.74%\n",
      "Epoch 14: Loss = 610.0146, Accuracy = 62.95%\n",
      "Epoch 15: Loss = 609.4628, Accuracy = 63.00%\n",
      "Epoch 16: Loss = 608.1498, Accuracy = 63.29%\n",
      "Epoch 17: Loss = 608.3541, Accuracy = 63.18%\n",
      "Epoch 18: Loss = 608.1173, Accuracy = 63.14%\n",
      "Epoch 19: Loss = 607.2159, Accuracy = 63.36%\n",
      "Epoch 20: Loss = 606.9482, Accuracy = 63.39%\n",
      "Epoch 21: Loss = 606.6182, Accuracy = 63.46%\n",
      "Epoch 22: Loss = 607.2127, Accuracy = 63.42%\n",
      "Epoch 23: Loss = 604.9519, Accuracy = 64.36%\n",
      "Epoch 24: Loss = 601.2295, Accuracy = 65.74%\n",
      "Epoch 25: Loss = 596.2074, Accuracy = 67.05%\n",
      "Epoch 26: Loss = 596.0917, Accuracy = 67.04%\n",
      "Epoch 27: Loss = 595.1656, Accuracy = 67.21%\n",
      "Epoch 28: Loss = 595.5876, Accuracy = 67.08%\n",
      "Epoch 29: Loss = 592.6865, Accuracy = 67.78%\n",
      "Epoch 30: Loss = 592.6442, Accuracy = 67.77%\n",
      "Epoch 31: Loss = 592.4631, Accuracy = 67.74%\n",
      "Epoch 32: Loss = 592.6935, Accuracy = 67.73%\n",
      "Epoch 33: Loss = 591.4458, Accuracy = 68.03%\n",
      "Epoch 34: Loss = 591.7266, Accuracy = 67.87%\n",
      "Epoch 35: Loss = 590.9844, Accuracy = 68.05%\n",
      "Epoch 36: Loss = 590.3718, Accuracy = 68.21%\n",
      "Epoch 37: Loss = 588.7144, Accuracy = 68.59%\n",
      "Epoch 38: Loss = 589.7120, Accuracy = 68.35%\n",
      "Epoch 39: Loss = 589.7344, Accuracy = 68.37%\n",
      "Epoch 40: Loss = 587.6300, Accuracy = 68.82%\n",
      "Epoch 41: Loss = 587.9805, Accuracy = 68.72%\n",
      "Epoch 42: Loss = 587.5246, Accuracy = 68.79%\n",
      "Epoch 43: Loss = 588.6910, Accuracy = 68.53%\n",
      "Epoch 44: Loss = 587.7480, Accuracy = 68.74%\n",
      "Epoch 45: Loss = 587.3196, Accuracy = 68.84%\n",
      "Epoch 46: Loss = 588.0181, Accuracy = 68.69%\n",
      "Epoch 47: Loss = 586.4016, Accuracy = 69.07%\n",
      "Epoch 48: Loss = 587.7836, Accuracy = 68.70%\n",
      "Epoch 49: Loss = 586.3531, Accuracy = 69.03%\n",
      "Epoch 50: Loss = 587.4129, Accuracy = 68.80%\n",
      "Epoch 51: Loss = 586.8565, Accuracy = 68.90%\n",
      "Epoch 52: Loss = 586.0568, Accuracy = 69.08%\n",
      "Epoch 53: Loss = 586.8235, Accuracy = 68.94%\n",
      "Epoch 54: Loss = 587.1615, Accuracy = 68.87%\n",
      "Epoch 55: Loss = 585.1787, Accuracy = 69.26%\n",
      "Epoch 56: Loss = 585.5804, Accuracy = 69.21%\n",
      "Epoch 57: Loss = 586.0529, Accuracy = 69.08%\n",
      "Epoch 58: Loss = 585.9724, Accuracy = 69.12%\n",
      "Epoch 59: Loss = 585.0955, Accuracy = 69.32%\n",
      "Epoch 60: Loss = 586.5516, Accuracy = 68.93%\n",
      "Epoch 61: Loss = 584.8575, Accuracy = 69.35%\n",
      "Epoch 62: Loss = 584.1722, Accuracy = 69.51%\n",
      "Epoch 63: Loss = 584.8629, Accuracy = 69.36%\n",
      "Epoch 64: Loss = 585.1031, Accuracy = 69.25%\n",
      "Epoch 65: Loss = 585.8332, Accuracy = 69.16%\n",
      "Epoch 66: Loss = 585.8301, Accuracy = 69.12%\n",
      "Epoch 67: Loss = 583.9383, Accuracy = 69.55%\n",
      "Epoch 68: Loss = 584.9877, Accuracy = 69.32%\n",
      "Epoch 69: Loss = 584.1947, Accuracy = 69.51%\n",
      "Epoch 70: Loss = 585.3566, Accuracy = 69.22%\n",
      "Epoch 71: Loss = 585.0394, Accuracy = 69.27%\n",
      "Epoch 72: Loss = 583.9155, Accuracy = 69.55%\n",
      "Epoch 73: Loss = 584.5217, Accuracy = 69.41%\n",
      "Epoch 74: Loss = 584.8117, Accuracy = 69.33%\n",
      "Epoch 75: Loss = 583.5006, Accuracy = 69.62%\n",
      "Epoch 76: Loss = 583.6743, Accuracy = 69.60%\n",
      "Epoch 77: Loss = 583.7214, Accuracy = 69.62%\n",
      "Epoch 78: Loss = 584.8296, Accuracy = 69.33%\n",
      "Epoch 79: Loss = 583.3358, Accuracy = 69.66%\n",
      "Epoch 80: Loss = 583.5396, Accuracy = 69.64%\n",
      "Epoch 81: Loss = 583.4313, Accuracy = 69.68%\n",
      "Epoch 82: Loss = 584.2302, Accuracy = 69.45%\n",
      "Epoch 83: Loss = 584.5576, Accuracy = 69.37%\n",
      "Epoch 84: Loss = 583.2216, Accuracy = 69.69%\n",
      "Epoch 85: Loss = 583.0521, Accuracy = 69.73%\n",
      "Epoch 86: Loss = 583.7402, Accuracy = 69.57%\n",
      "Epoch 87: Loss = 583.8801, Accuracy = 69.55%\n",
      "Epoch 88: Loss = 582.9753, Accuracy = 69.75%\n",
      "Epoch 89: Loss = 582.8864, Accuracy = 69.78%\n",
      "Epoch 90: Loss = 583.0776, Accuracy = 69.72%\n",
      "Epoch 91: Loss = 583.6059, Accuracy = 69.60%\n",
      "Epoch 92: Loss = 583.2979, Accuracy = 69.69%\n",
      "Epoch 93: Loss = 582.6541, Accuracy = 69.81%\n",
      "Epoch 94: Loss = 582.6541, Accuracy = 69.83%\n",
      "Epoch 95: Loss = 583.2150, Accuracy = 69.70%\n",
      "Epoch 96: Loss = 583.1063, Accuracy = 69.70%\n",
      "Epoch 97: Loss = 582.4836, Accuracy = 69.87%\n",
      "Epoch 98: Loss = 583.0183, Accuracy = 69.73%\n",
      "Epoch 99: Loss = 582.5633, Accuracy = 69.84%\n",
      "Epoch 100: Loss = 583.4063, Accuracy = 69.62%\n",
      "Model model_odd_even_trained trained and saved as .pkl.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_model(train_file, test_file, target_column, model_name, epochs=100, batch_size=256, lr=0.0003):\n",
    "    train_dataset = LotteryDataset(train_file, target_column)\n",
    "    test_dataset = LotteryDataset(test_file, target_column)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = MLP(input_size=train_dataset.X.shape[1])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch).squeeze()\n",
    "            loss = criterion(output, y_batch.float())  # Convert target to Float\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            correct += ((output > 0.5) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        accuracy = (correct / total) * 100\n",
    "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "    \n",
    "    with open(f\"{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model {model_name} trained and saved as .pkl.\")\n",
    "    exit()  # Stops script after saving\n",
    "\n",
    "\n",
    "# Train Models\n",
    "train_model(\"train.csv\", \"test.csv\", \"odd_even_1\", \"model_odd_even_trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\"train.csv\", \"test.csv\", \"big_small_1\", \"model_big_small_trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     evaluate_model(model, test_loader, criterion)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_odd_even_trained.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43modd_even_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m load_and_evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_big_small_trained.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_small_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 34\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[1;34m(model_path, test_file, target_column)\u001b[0m\n\u001b[0;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     33\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader, criterion)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (X_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m      8\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert target to Float\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((output \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\jepto\\Documents\\Projects\\lucky28 pro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jepto\\Documents\\Projects\\lucky28 pro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jepto\\Documents\\Projects\\lucky28 pro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jepto\\Documents\\Projects\\lucky28 pro\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3567\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    batch_accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for i, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            output = model(X_batch).squeeze()\n",
    "            loss = criterion(output, y_batch.float())  # Convert target to Float\n",
    "            total_loss += loss.item()\n",
    "            correct += ((output > 0.5) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            \n",
    "            # Compute accuracy for every 10 draws\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_accuracy = (correct / total) * 100\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "                correct, total = 0, 0  # Reset for next batch\n",
    "    \n",
    "    overall_accuracy = sum(batch_accuracies) / len(batch_accuracies)\n",
    "    print(f\"Overall Test Accuracy (100 draws): {overall_accuracy:.2f}%\")\n",
    "    for i, acc in enumerate(batch_accuracies):\n",
    "        print(f\"Accuracy for rounds {i*10+1}-{(i+1)*10}: {acc:.2f}%\")\n",
    "\n",
    "# Load trained models and evaluate\n",
    "def load_and_evaluate(model_path, test_file, target_column):\n",
    "    test_dataset = LotteryDataset(test_file, target_column)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "# Evaluate models\n",
    "load_and_evaluate(\"model_odd_even_trained.pkl\", \"test.csv\", \"odd_even_1\")\n",
    "load_and_evaluate(\"model_big_small_trained.pkl\", \"test.csv\", \"big_small_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
